{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import torch.multiprocessing as mp\n",
    "import datasets\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import json\n",
    "import types\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "\n",
    "from kmeans_gpu import KMeansPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../../huggingface-models/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16, _attn_implementation=\"eager\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "num_key_value_heads = model.config.num_key_value_heads\n",
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "head_dim = model.config.head_dim\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_num_tokens = 64 * 1024\n",
    "target_num_random_tokens = 16 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../../huggingface-datasets/fineweb-edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(dataset_dir, data_files=\"sample/10BT/000_00000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_attention_forward_wrapper(query_states_stored, key_states_stored):\n",
    "    rerope_relative_pos = 2048\n",
    "\n",
    "    def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "        cos = cos.unsqueeze(unsqueeze_dim)\n",
    "        sin = sin.unsqueeze(unsqueeze_dim)\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin) if q is not None else None\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin) if k is not None else None\n",
    "        return q_embed, k_embed\n",
    "\n",
    "    def llama_attention_forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        device = hidden_states.device\n",
    "        dtype = hidden_states.dtype\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
    "            query_slices = self.q_proj.weight.split(\n",
    "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
    "            )\n",
    "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
    "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
    "\n",
    "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            query_states = torch.cat(query_states, dim=-1)\n",
    "\n",
    "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            key_states = torch.cat(key_states, dim=-1)\n",
    "\n",
    "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            value_states = torch.cat(value_states, dim=-1)\n",
    "\n",
    "        else:\n",
    "            query_states = self.q_proj(hidden_states)\n",
    "            key_states = self.k_proj(hidden_states)\n",
    "            value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos_rerope, sin_rerope = self.rotary_emb(value_states, torch.tensor([rerope_relative_pos], device=device, dtype=torch.long).unsqueeze(0))\n",
    "        query_states_rerope, _ = apply_rotary_pos_emb(query_states, None, cos_rerope, sin_rerope)\n",
    "\n",
    "        key_states_stored[self.layer_idx] = torch.cat((key_states_stored[self.layer_idx], key_states[0].detach().to(\"cpu\")), dim=-2)\n",
    "        query_states_stored[self.layer_idx] = torch.cat((query_states_stored[self.layer_idx], query_states_rerope[0].detach().to(\"cpu\")), dim=-2)\n",
    "\n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
    "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
    "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
    "        else:\n",
    "            attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "    \n",
    "    return llama_attention_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states_stored = [torch.zeros((num_key_value_heads, 0, head_dim), dtype=torch.bfloat16) for _ in range(num_hidden_layers)]\n",
    "query_states_stored = [torch.zeros((num_attention_heads, 0, head_dim), dtype=torch.bfloat16) for _ in range(num_hidden_layers)]\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = types.MethodType(llama_attention_forward_wrapper(query_states_stored, key_states_stored), layer.self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 8192\n",
    "max_len = 16 * 1024\n",
    "num_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(desc=\"[INFO] collecting key states\", total=target_num_tokens) as pbar:\n",
    "        for sample_idx in range(len(dataset[\"train\"])):\n",
    "            if num_tokens >= target_num_tokens:\n",
    "                break\n",
    "            \n",
    "            input_ids = tokenizer.encode(dataset[\"train\"][sample_idx][\"text\"])\n",
    "            input_ids = input_ids[:max_len]\n",
    "\n",
    "            kv_cache = DynamicCache()\n",
    "            for chunk_begin in range(0, len(input_ids), chunk_size):\n",
    "                chunk_end = min(chunk_begin + chunk_size, len(input_ids))\n",
    "                kv_cache = model(torch.tensor(input_ids[chunk_begin:chunk_end], dtype=torch.long).unsqueeze(0).to(\"cuda\"), past_key_values=kv_cache).past_key_values\n",
    "            \n",
    "            pbar.update(len(input_ids))\n",
    "\n",
    "            num_tokens += len(input_ids)\n",
    "\n",
    "print(f\"[INFO] processed {sample_idx} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_uuid = 128 * 1024\n",
    "random_string = \", \".join([str(uuid.uuid4()) for _ in range(num_uuid)])\n",
    "\n",
    "random_string[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 2048\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(random_string)\n",
    "    input_ids = input_ids[:target_num_random_tokens]\n",
    "\n",
    "    kv_cache = DynamicCache()\n",
    "    for chunk_begin in tqdm(range(0, len(input_ids), chunk_size), desc=\"collecting key states on random string\"):\n",
    "        chunk_end = min(chunk_begin + chunk_size, len(input_ids))\n",
    "        kv_cache = model(torch.tensor(input_ids[chunk_begin:chunk_end], dtype=torch.long).unsqueeze(0).to(\"cuda\"), past_key_values=kv_cache).past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "\n",
    "cholesky_factors = [torch.zeros(num_key_value_heads, head_dim, head_dim, dtype=dtype) for _ in range(num_hidden_layers)]\n",
    "inv_cholesky_factors = [torch.zeros(num_key_value_heads, head_dim, head_dim, dtype=dtype) for _ in range(num_hidden_layers)]\n",
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "\n",
    "for layer_idx in tqdm(range(num_hidden_layers)):\n",
    "    for key_value_head_idx in range(num_key_value_heads):\n",
    "        q = query_states_stored[layer_idx][num_key_value_groups*key_value_head_idx:num_key_value_groups*(key_value_head_idx+1)].reshape(-1, head_dim).to(torch.float32)\n",
    "        H = q.T @ q\n",
    "        H = H / (H**2).mean().sqrt()\n",
    "        L = torch.linalg.cholesky(H + epsilon * torch.eye(H.size(0)))\n",
    "        inv_cholesky_factors[layer_idx][key_value_head_idx] = torch.linalg.inv(L)\n",
    "        cholesky_factors[layer_idx][key_value_head_idx] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_size = 4096\n",
    "\n",
    "codebooks = [torch.zeros((num_key_value_heads, codebook_size, head_dim)) for _ in range(num_hidden_layers)]\n",
    "\n",
    "for layer_idx in range(num_hidden_layers):\n",
    "\n",
    "    for key_value_head_idx in tqdm(range(model.config.num_key_value_heads), desc=f\"Layer {layer_idx}\"):\n",
    "        kmeans = KMeansPlusPlus(n_clusters=codebook_size, device=\"cuda\")\n",
    "        kmeans.fit(key_states_stored[layer_idx][key_value_head_idx] @ cholesky_factors[layer_idx][key_value_head_idx])\n",
    "        codebooks[layer_idx][key_value_head_idx] = kmeans.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((cholesky_factors, inv_cholesky_factors), \"../codebooks/llama-3-8b-cholesky_factors.pt\")\n",
    "torch.save(codebooks, \"../codebooks/llama-3-8b-codebooks.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqkey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
